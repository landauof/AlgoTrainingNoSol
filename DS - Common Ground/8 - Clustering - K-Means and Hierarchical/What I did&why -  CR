

1. Parse the target column as requested
2. Read the data and dropped columns that instructions said should be dropped
3. Looked at the names of columns & their meaning using reddit and stuff (e.g. reddit.com/r/FIFA/comments/3znwub/beginners_guide_work_rates/)
    * Some features are irrelevant (club, date of birth, full name...)
    * Some feature contained in others (international_reputation gives bonuses to stuff...)
    * Some features need parsing (work_rate, player_tags, body_type...)
4. Removed features that feels irrelevant or encapsulated in other features
5. Parsed features that needed to be parsed:
    * tags, traits, body type...
    CR - what did U do with them? one hot encoding? something else?
6 Turned body_type and strong foot to numeric:
    * Filled special body types using "domain knowledge..."
7. Handle missing values:
    * national jersey shirt: since most of the players don't play in their national teams, decided to drop this feature
    * team jersy shirt: if a player has no team, it means he appears only on national team-> use this number
    * release clause: filled with 0 because it should be 0 if not stated otherwise (LAWYERED.) CR - AHBAL.
8. Turned float cols to ints (after making sure there is no data lost) and reindex df using URLs

At this point, I think the most indicative features will be the numeric scores (defending, shooting...), the scores per position (rw,lw...) and the binary tags (#Tackling...)

9. Tried K-Means with all (115) features
    * Used PCA to plot results and it looks pretty good
    * used tsne to plot results and it looks pretty bad
    * silhouette_score = 0.1596 (however, not sure yet this is the most fitting score)
10. Same as 9 but using mean/std normalization
    * Used PCA to plot results and it looks pretty good
    * used tsne to plot results and it looks pretty bad
    * silhouette_score = 0.1637 (however, not sure yet this is the most fitting score)
11. Build different datasets for experimenting:
    * Full df - PCA
    * Full df - t-SNE
    * tags only
    * traits only
    * position score only
    CR - ok... interesting, maybe weighting the different features is better? IDK...
12. After some online digging, found out that each position has a unique coefficient multiplier table
    * After calculating them using FIFA19's formulas, saw that I have the ones of 2020 as features and that they are not a perfect fit. CR - what?
13. Using different datasets (without dimentional reduction at the clustering)
    * Using position scores - similar to the full df results (sil 0.390)
    * tags - put almost everyone in the same cluster (sil 0.956) - CR - good
    * traits - put almost everyont in the same cluster (sil 0.519) - CR - good again.
    * general info - kind of like position scores but not as good (sil 0.143) - CR - why not insert them both together?
    
At this point, I think the tags and traits are problematic because they appear only with famous players (EA being lazy - CR - yes, but it still costs 400 NIS to play). In addition, looking at the data, relatively good players (regardless of their natural position) appears togheter. I think this happens because their scores are better than bad players at their natural position (e.g. a world-class defender can play as a relatively good midfielder, so he is similar to regular midfielder)

14. Try normalizing the values of the position scores by dividing them in the overall score
15. Run KMeans with position scores
    * Graph looks good (and different than before), (sil 0.547) - CR - great idea. we talked about it



16. Got and parsed labels for 15%
    * Current adjusted AMI score (for the 15%) = 0.638 - CR- what is the AMI score?
    * accuracy score (for the 15%) =  0.876
    * Looking at the confusion matrix, the main issue is that I predict MD too often
17. Tried using DBSCAN instead of kmeans (similar configuration as in 15)  
    * Results look not so good - CR - DBSCAN requires a fine hyperopt
        ** It is kind of expected because the data is very "close" (there are no clear clusters because the players can be good at more than one position)
        ** Further, the algorithm calculated the amount of clusters on its own, and can leave outliers out, which is not our scenario - CR - agreed
18. Tried Spectral Clustering (like 15)
    * Got similar results to KMeans (slightly different tough) (sil 0.535)
    * When adding evaluation using the labeled data got acc=0.853 with the same issue of 16 but slightly worse
19. Trying self learning for shits and giggles: sklearn's LabelPropagation (using rbf and knn) CR - rbf???
    * Bad results (knn wont run, rbf gets only two classes)  
20. Implemented self training and tried a decision tree using max_depth=5, confidence threshold=0.75
    CR - what about RF? XGBOOST? lightGBM? CATBOOST?!?!?! throw in some SOTA's
    * Accuracy~0.93 (using 0.5 of the labeled for validation and the rest as initial labeled training set)
    * Seems like I still predict MD too often

At this point, the results look fairly nice. Altough I want to keep trying more methods, I think the results won't improve by much
FUTURE DIRECTIONS:
    - Give another shot to different clustering methods at part A
    - Try different models for self training - CR- self training is a single method :) U mean semi-supervised
    - Try different semi-supervised methods (M-training...) CR - OK U didn't mean semi-supervised... explain what different self training methods there R (consider this as homework)
    - Look for features that better distinguish between ST and B (looking at features that are not the position scores) - CR - why specifically ST and B? I thought Ur problems R the MD and ST right?